{"ast":null,"code":"\"use strict\";\n\nvar __extends = this && this.__extends || function () {\n  var extendStatics = Object.setPrototypeOf || {\n    __proto__: []\n  } instanceof Array && function (d, b) {\n    d.__proto__ = b;\n  } || function (d, b) {\n    for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p];\n  };\n\n  return function (d, b) {\n    extendStatics(d, b);\n\n    function __() {\n      this.constructor = d;\n    }\n\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n  };\n}();\n\nvar __assign = this && this.__assign || Object.assign || function (t) {\n  for (var s, i = 1, n = arguments.length; i < n; i++) {\n    s = arguments[i];\n\n    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p)) t[p] = s[p];\n  }\n\n  return t;\n};\n\nvar __awaiter = this && this.__awaiter || function (thisArg, _arguments, P, generator) {\n  return new (P || (P = Promise))(function (resolve, reject) {\n    function fulfilled(value) {\n      try {\n        step(generator.next(value));\n      } catch (e) {\n        reject(e);\n      }\n    }\n\n    function rejected(value) {\n      try {\n        step(generator[\"throw\"](value));\n      } catch (e) {\n        reject(e);\n      }\n    }\n\n    function step(result) {\n      result.done ? resolve(result.value) : new P(function (resolve) {\n        resolve(result.value);\n      }).then(fulfilled, rejected);\n    }\n\n    step((generator = generator.apply(thisArg, _arguments || [])).next());\n  });\n};\n\nvar __generator = this && this.__generator || function (thisArg, body) {\n  var _ = {\n    label: 0,\n    sent: function () {\n      if (t[0] & 1) throw t[1];\n      return t[1];\n    },\n    trys: [],\n    ops: []\n  },\n      f,\n      y,\n      t,\n      g;\n  return g = {\n    next: verb(0),\n    \"throw\": verb(1),\n    \"return\": verb(2)\n  }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function () {\n    return this;\n  }), g;\n\n  function verb(n) {\n    return function (v) {\n      return step([n, v]);\n    };\n  }\n\n  function step(op) {\n    if (f) throw new TypeError(\"Generator is already executing.\");\n\n    while (_) try {\n      if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n      if (y = 0, t) op = [op[0] & 2, t.value];\n\n      switch (op[0]) {\n        case 0:\n        case 1:\n          t = op;\n          break;\n\n        case 4:\n          _.label++;\n          return {\n            value: op[1],\n            done: false\n          };\n\n        case 5:\n          _.label++;\n          y = op[1];\n          op = [0];\n          continue;\n\n        case 7:\n          op = _.ops.pop();\n\n          _.trys.pop();\n\n          continue;\n\n        default:\n          if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) {\n            _ = 0;\n            continue;\n          }\n\n          if (op[0] === 3 && (!t || op[1] > t[0] && op[1] < t[3])) {\n            _.label = op[1];\n            break;\n          }\n\n          if (op[0] === 6 && _.label < t[1]) {\n            _.label = t[1];\n            t = op;\n            break;\n          }\n\n          if (t && _.label < t[2]) {\n            _.label = t[2];\n\n            _.ops.push(op);\n\n            break;\n          }\n\n          if (t[2]) _.ops.pop();\n\n          _.trys.pop();\n\n          continue;\n      }\n\n      op = body.call(thisArg, _);\n    } catch (e) {\n      op = [6, e];\n      y = 0;\n    } finally {\n      f = t = 0;\n    }\n\n    if (op[0] & 5) throw op[1];\n    return {\n      value: op[0] ? op[1] : void 0,\n      done: true\n    };\n  }\n};\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\n\nvar core_1 = require(\"@aws-amplify/core\");\n\nvar storage_1 = require(\"@aws-amplify/storage\");\n\nvar Providers_1 = require(\"../types/Providers\");\n\nvar Rekognition = require(\"aws-sdk/clients/rekognition\");\n\nvar types_1 = require(\"../types\");\n\nvar Textract = require(\"aws-sdk/clients/textract\");\n\nvar Utils_1 = require(\"./Utils\");\n\nvar IdentifyTextUtils_1 = require(\"./IdentifyTextUtils\");\n\nvar AmazonAIIdentifyPredictionsProvider =\n/** @class */\nfunction (_super) {\n  __extends(AmazonAIIdentifyPredictionsProvider, _super);\n\n  function AmazonAIIdentifyPredictionsProvider() {\n    return _super.call(this) || this;\n  }\n\n  AmazonAIIdentifyPredictionsProvider.prototype.getProviderName = function () {\n    return 'AmazonAIIdentifyPredictionsProvider';\n  };\n  /**\n   * Verify user input source and converts it into source object readable by Rekognition and Textract.\n   * Note that Rekognition and Textract use the same source interface, so we need not worry about types.\n   * @param {IdentifySource} source - User input source that directs to the object user wants\n   * to identify (storage, file, or bytes).\n   * @return {Promise<Rekognition.Image>} - Promise resolving to the converted source object.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.configureSource = function (source) {\n    return new Promise(function (res, rej) {\n      if (types_1.isStorageSource(source)) {\n        var storageConfig = {\n          level: source.level,\n          identityId: source.identityId\n        };\n        storage_1.default.get(source.key, storageConfig).then(function (url) {\n          var parser = /https:\\/\\/([a-zA-Z0-9%-_.]+)\\.s3\\.[A-Za-z0-9%-._~]+\\/([a-zA-Z0-9%-._~/]+)\\?/;\n          var parsedURL = url.match(parser);\n          if (parsedURL.length < 3) rej('Invalid S3 key was given.');\n          res({\n            S3Object: {\n              Bucket: parsedURL[1],\n              Name: parsedURL[2]\n            }\n          });\n        }).catch(function (err) {\n          return rej(err);\n        });\n      } else if (types_1.isFileSource(source)) {\n        Utils_1.blobToArrayBuffer(source.file).then(function (buffer) {\n          res({\n            Bytes: buffer\n          });\n        }).catch(function (err) {\n          return rej(err);\n        });\n      } else if (types_1.isBytesSource(source)) {\n        var bytes = source.bytes;\n\n        if (bytes instanceof Blob) {\n          Utils_1.blobToArrayBuffer(bytes).then(function (buffer) {\n            res({\n              Bytes: buffer\n            });\n          }).catch(function (err) {\n            return rej(err);\n          });\n        } // everything else can be directly passed to Rekognition / Textract.\n\n\n        res({\n          Bytes: bytes\n        });\n      } else {\n        rej('Input source is not configured correctly.');\n      }\n    });\n  };\n  /**\n   * Recognize text from real-world images and documents (plain text, forms and tables). Detects text in the input\n   * image and converts it into machine-readable text.\n   * @param {IdentifySource} source - Object containing the source image and feature types to analyze.\n   * @return {Promise<IdentifyTextOutput>} - Promise resolving to object containing identified texts.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyText = function (input) {\n    var _this = this;\n\n    return new Promise(function (res, rej) {\n      return __awaiter(_this, void 0, void 0, function () {\n        var credentials, _a, _b, _c, region, _d, _e, configFormat, inputDocument, format, featureTypes, textractParam_1, rekognitionParam, param;\n\n        var _this = this;\n\n        return __generator(this, function (_f) {\n          switch (_f.label) {\n            case 0:\n              return [4\n              /*yield*/\n              , core_1.Credentials.get()];\n\n            case 1:\n              credentials = _f.sent();\n              if (!credentials) return [2\n              /*return*/\n              , rej('No credentials')];\n              _a = this._config.identifyText, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.defaults, _e = (_d === void 0 ? {} : _d).format, configFormat = _e === void 0 ? 'PLAIN' : _e;\n              this.rekognition = new Rekognition({\n                region: region,\n                credentials: credentials\n              });\n              this.textract = new Textract({\n                region: region,\n                credentials: credentials\n              });\n              return [4\n              /*yield*/\n              , this.configureSource(input.text.source).then(function (data) {\n                return inputDocument = data;\n              }).catch(function (err) {\n                rej(err);\n              })];\n\n            case 2:\n              _f.sent();\n\n              format = input.text.format || configFormat;\n              featureTypes = [];\n              if (format === 'FORM' || format === 'ALL') featureTypes.push('FORMS');\n              if (format === 'TABLE' || format === 'ALL') featureTypes.push('TABLES');\n\n              if (featureTypes.length === 0) {\n                textractParam_1 = {\n                  Document: inputDocument\n                };\n                rekognitionParam = {\n                  Image: inputDocument\n                };\n                this.rekognition.detectText(rekognitionParam, function (rekognitionErr, rekognitionData) {\n                  if (rekognitionErr) return rej(rekognitionErr);\n                  var rekognitionResponse = IdentifyTextUtils_1.categorizeRekognitionBlocks(rekognitionData.TextDetections);\n\n                  if (rekognitionResponse.text.words.length < 50) {\n                    // did not hit the word limit, return the data\n                    return res(rekognitionResponse);\n                  }\n\n                  _this.textract.detectDocumentText(textractParam_1, function (textractErr, textractData) {\n                    if (textractErr) return rej(textractErr); // use the service that identified more texts.\n\n                    if (rekognitionData.TextDetections.length > textractData.Blocks.length) {\n                      return res(rekognitionResponse);\n                    } else {\n                      return res(IdentifyTextUtils_1.categorizeTextractBlocks(textractData.Blocks));\n                    }\n                  });\n                });\n              } else {\n                param = {\n                  Document: inputDocument,\n                  FeatureTypes: featureTypes\n                };\n                this.textract.analyzeDocument(param, function (err, data) {\n                  if (err) return rej(err);\n                  var blocks = data.Blocks;\n                  res(IdentifyTextUtils_1.categorizeTextractBlocks(blocks));\n                });\n              }\n\n              return [2\n              /*return*/\n              ];\n          }\n        });\n      });\n    });\n  };\n  /**\n   * Identify instances of real world entities from an image and if it contains unsafe content.\n   * @param {IdentifyLabelsInput} input - Object containing the source image and entity type to identify.\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to an array of identified entities.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyLabels = function (input) {\n    var _this = this;\n\n    return new Promise(function (res, rej) {\n      return __awaiter(_this, void 0, void 0, function () {\n        var credentials, _a, _b, _c, region, _d, _e, type, inputImage, param, servicePromises, entityType;\n\n        return __generator(this, function (_f) {\n          switch (_f.label) {\n            case 0:\n              return [4\n              /*yield*/\n              , core_1.Credentials.get()];\n\n            case 1:\n              credentials = _f.sent();\n              if (!credentials) return [2\n              /*return*/\n              , rej('No credentials')];\n              _a = this._config.identifyLabels, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.defaults, _e = (_d === void 0 ? {} : _d).type, type = _e === void 0 ? 'LABELS' : _e;\n              this.rekognition = new Rekognition({\n                region: region,\n                credentials: credentials\n              });\n              return [4\n              /*yield*/\n              , this.configureSource(input.labels.source).then(function (data) {\n                inputImage = data;\n              }).catch(function (err) {\n                return rej(err);\n              })];\n\n            case 2:\n              _f.sent();\n\n              param = {\n                Image: inputImage\n              };\n              servicePromises = [];\n              entityType = input.labels.type || type;\n\n              if (entityType === 'LABELS' || entityType === 'ALL') {\n                servicePromises.push(this.detectLabels(param));\n              }\n\n              if (entityType === 'UNSAFE' || entityType === 'ALL') {\n                servicePromises.push(this.detectModerationLabels(param));\n              } // if (servicePromises.length === 0) {\n              //     rej('You must specify entity type: LABELS | UNSAFE | ALL');\n              // }\n\n\n              Promise.all(servicePromises).then(function (data) {\n                var identifyResult = {}; // concatenate resolved promises to a single object\n\n                data.forEach(function (val) {\n                  identifyResult = __assign({}, identifyResult, val);\n                });\n                res(identifyResult);\n              }).catch(function (err) {\n                return rej(err);\n              });\n              return [2\n              /*return*/\n              ];\n          }\n        });\n      });\n    });\n  };\n  /**\n   * Calls Rekognition.detectLabels and organizes the returned data.\n   * @param {Rekognition.DetectLabelsRequest} param - parameter to be passed onto Rekognition\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to organized detectLabels response.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.detectLabels = function (param) {\n    var _this = this;\n\n    return new Promise(function (res, rej) {\n      _this.rekognition.detectLabels(param, function (err, data) {\n        if (err) return rej(err);\n        if (!data.Labels) return res({\n          labels: null\n        }); // no image was detected\n\n        var detectLabelData = data.Labels.map(function (val) {\n          var boxes = val.Instances ? val.Instances.map(function (val) {\n            return Utils_1.makeCamelCase(val.BoundingBox);\n          }) : undefined;\n          return {\n            name: val.Name,\n            boundingBoxes: boxes,\n            metadata: {\n              confidence: val.Confidence,\n              parents: Utils_1.makeCamelCaseArray(val.Parents)\n            }\n          };\n        });\n        return res({\n          labels: detectLabelData\n        });\n      });\n    });\n  };\n  /**\n   * Calls Rekognition.detectModerationLabels and organizes the returned data.\n   * @param {Rekognition.DetectLabelsRequest} param - Parameter to be passed onto Rekognition\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to organized detectModerationLabels response.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.detectModerationLabels = function (param) {\n    var _this = this;\n\n    return new Promise(function (res, rej) {\n      _this.rekognition.detectModerationLabels(param, function (err, data) {\n        if (err) return rej(err);\n\n        if (data.ModerationLabels.length !== 0) {\n          return res({\n            unsafe: 'YES'\n          });\n        } else {\n          return res({\n            unsafe: 'NO'\n          });\n        }\n      });\n    });\n  };\n  /**\n   * Identify faces within an image that is provided as input, and match faces from a collection\n   * or identify celebrities.\n   * @param {IdentifyEntityInput} input - object containing the source image and face match options.\n   * @return {Promise<IdentifyEntityOutput>} Promise resolving to identify results.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyEntities = function (input) {\n    var _this = this;\n\n    return new Promise(function (res, rej) {\n      return __awaiter(_this, void 0, void 0, function () {\n        var credentials, _a, _b, _c, region, _d, celebrityDetectionEnabled, _e, _f, _g, collectionIdConfig, _h, maxFacesConfig, inputImage, param, _j, _k, collectionId, _l, maxFaces, updatedParam;\n\n        var _this = this;\n\n        return __generator(this, function (_m) {\n          switch (_m.label) {\n            case 0:\n              return [4\n              /*yield*/\n              , core_1.Credentials.get()];\n\n            case 1:\n              credentials = _m.sent();\n              if (!credentials) return [2\n              /*return*/\n              , rej('No credentials')];\n              _a = this._config.identifyEntities, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.celebrityDetectionEnabled, celebrityDetectionEnabled = _d === void 0 ? false : _d, _e = _b.defaults, _f = _e === void 0 ? {} : _e, _g = _f.collectionId, collectionIdConfig = _g === void 0 ? '' : _g, _h = _f.maxEntities, maxFacesConfig = _h === void 0 ? 50 : _h; // default arguments\n\n              this.rekognition = new Rekognition({\n                region: region,\n                credentials: credentials\n              });\n              return [4\n              /*yield*/\n              , this.configureSource(input.entities.source).then(function (data) {\n                return inputImage = data;\n              }).catch(function (err) {\n                return rej(err);\n              })];\n\n            case 2:\n              _m.sent();\n\n              param = {\n                Image: inputImage\n              };\n\n              if (types_1.isIdentifyCelebrities(input.entities) && input.entities.celebrityDetection) {\n                if (!celebrityDetectionEnabled) {\n                  return [2\n                  /*return*/\n                  , rej('Error: You have to enable celebrity detection first')];\n                }\n\n                this.rekognition.recognizeCelebrities(param, function (err, data) {\n                  if (err) return rej(err);\n                  var faces = data.CelebrityFaces.map(function (celebrity) {\n                    return {\n                      boundingBox: Utils_1.makeCamelCase(celebrity.Face.BoundingBox),\n                      landmarks: Utils_1.makeCamelCaseArray(celebrity.Face.Landmarks),\n                      metadata: __assign({}, Utils_1.makeCamelCase(celebrity, ['Id', 'Name', 'Urls']), {\n                        pose: Utils_1.makeCamelCase(celebrity.Face.Pose)\n                      })\n                    };\n                  });\n                  res({\n                    entities: faces\n                  });\n                });\n              } else if (types_1.isIdentifyFromCollection(input.entities) && input.entities.collection) {\n                _j = input.entities, _k = _j.collectionId, collectionId = _k === void 0 ? collectionIdConfig : _k, _l = _j.maxEntities, maxFaces = _l === void 0 ? maxFacesConfig : _l;\n                updatedParam = __assign({}, param, {\n                  CollectionId: collectionId,\n                  MaxFaces: maxFaces\n                });\n                this.rekognition.searchFacesByImage(updatedParam, function (err, data) {\n                  if (err) return rej(err);\n                  var faces = data.FaceMatches.map(function (val) {\n                    return {\n                      boundingBox: Utils_1.makeCamelCase(val.Face.BoundingBox),\n                      metadata: {\n                        externalImageId: _this.decodeExternalImageId(val.Face.ExternalImageId),\n                        similarity: val.Similarity\n                      }\n                    };\n                  });\n                  res({\n                    entities: faces\n                  });\n                });\n              } else {\n                this.rekognition.detectFaces(param, function (err, data) {\n                  if (err) return rej(err);\n                  var faces = data.FaceDetails.map(function (detail) {\n                    // face attributes keys we want to extract from Rekognition's response\n                    var attributeKeys = ['Smile', 'Eyeglasses', 'Sunglasses', 'Gender', 'Beard', 'Mustache', 'EyesOpen', 'MouthOpen'];\n                    var faceAttributes = Utils_1.makeCamelCase(detail, attributeKeys);\n\n                    if (detail.Emotions) {\n                      faceAttributes['emotions'] = detail.Emotions.map(function (emotion) {\n                        return emotion.Type;\n                      });\n                    }\n\n                    return {\n                      boundingBox: Utils_1.makeCamelCase(detail.BoundingBox),\n                      landmarks: Utils_1.makeCamelCaseArray(detail.Landmarks),\n                      ageRange: Utils_1.makeCamelCase(detail.AgeRange),\n                      attributes: Utils_1.makeCamelCase(detail, attributeKeys),\n                      metadata: {\n                        confidence: detail.Confidence,\n                        pose: Utils_1.makeCamelCase(detail.Pose)\n                      }\n                    };\n                  });\n                  res({\n                    entities: faces\n                  });\n                });\n              }\n\n              return [2\n              /*return*/\n              ];\n          }\n        });\n      });\n    });\n  };\n\n  AmazonAIIdentifyPredictionsProvider.prototype.decodeExternalImageId = function (externalImageId) {\n    return ('' + externalImageId).replace(/::/g, '/');\n  };\n\n  return AmazonAIIdentifyPredictionsProvider;\n}(Providers_1.AbstractIdentifyPredictionsProvider);\n\nexports.default = AmazonAIIdentifyPredictionsProvider;","map":null,"metadata":{},"sourceType":"script"}